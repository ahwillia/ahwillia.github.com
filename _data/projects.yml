- title: Unsupervised learning methods for large-scale, multi-trial neural data
  description: >
    A common paradigm in systems neuroscience is to simultaneously record the
    activity of many neurons over repeated experimental trials (e.g., multiple
    presentations of a sensory stimulus, or a repeated motor action). The resulting
    datasets can be very large, potentially containing thousands of
    neurons over thousands of trials. I'm interested in finding general-purpose
    statistical approaches for understanding datasets of this form. 
- items:
  - name: Tensor Decomposition
    icon: file-pdf-o
    url: https://pyramidal.stanford.edu/publications/Williams2018_Neuron.pdf
    description: >
      In collaboration with experimental groups headed by 
      <a href="https://shenoy.people.stanford.edu/overview" target="_blank">Krishna Shenoy</a>
      and 
      <a href="https://pyramidal.stanford.edu/" target="_blank">Mark Schnitzer</a>
      we demonstrate how to extract functional neural subpopulations across learning
      and changing environments
  - name: Time Warping
    icon: file-pdf-o
    url: /pdf/2020_timewarp.pdf
    description: >
      In collaboration with
      <a href="https://med.nyu.edu/faculty/dmitry-rinberg" target="_blank">Rinberg</a>,
      <a href="https://shenoy.people.stanford.edu/overview" target="_blank">Shenoy</a>,
      and 
      <a href="https://olveczkylab.oeb.harvard.edu/" target="_blank">&Ouml;lveczky</a>,
      labs we demonstrate how to discover precisely timed neural spike patterns in
      the presence of trial-to-trial jitter and other temporal misalignments.
  - name: Tutorial video
    icon: youtube-square
    url: https://www.youtube.com/watch?v=hmmnRF66hOA
    description: >
      In this talk (given at MIT, 2017) I give a high level overview of some of the research
      cited above.

- title: Automated segmentation and detection of motifs in high-dimensional time series
  description: >
    Trial-structured data (described above) are convenient for data analysis &mdash; in particular,
    if neural dynamics on different trials follow a similar (though not identical) trajectory,
    we can build a model that isolates this structure while discarding the remaining "noise"
    (for lack of a better term). A more challenging problem is how to extract scientifically
    interpretable features from unstructured neural and behavioral data streams (e.g. during
    unconstrained natural behaviors). To address this, I have developed some models that extract
    temporal motifs or "pseudo-trials" from raw time series.
- items:
  - name: Convolutional NMF
    icon: file-pdf-o
    url: https://doi.org/10.7554/eLife.38471
    description: >
      In collaboration with <a href="https://emackev.github.io/" target="_blank">Emily Mackevicius</a>,
      <a href="http://web.mit.edu/abahle/www/" target="_blank">Andrew Bahle</a>, 
      <a href="https://goldmanlab.faculty.ucdavis.edu/" target="_blank">Mark Goldman</a>,
      and <a href="https://feelaboratory.org/" target="_blank">Michale Fee</a>, I helped
      repurpose a convolutional nonnegative matrix factorization (convNMF) model (see
      <a href="https://doi.org/10.1109/TASL.2006.876726" target="_blank">Smaragdis, 2007</a>)
      to extract sequential firing patterns in songbirds and rats from unannotated neural population
      recordings. 
  - name: A Point Process Framework for Sequence Detection
    icon: file-pdf-o
    url: https://arxiv.org/pdf/2010.04875.pdf
    description: >
      Here we reformulate the convNMF model for spike train data, and in doing so draw a connection to
      <a href="https://www.jstor.org/stable/2983905" target="_blank">Neyman-Scott point process</a>
      models. We find that this model is more capable of handling noise in low firing rate regimes
      than the original convNMF model. We also get better quantification of model uncertainty by performing
      parameter inference in a fully Bayesian manner.


- title: Modeling individual variability in biological and model networks
  description: >
    Comparative analyses are a well-established approach used in many fields
    of biology including anatomy, ecology, and cellular physiology. Applying
    similar approaches in systems-level neuroscience is challenging because we
    often lack a principled mapping between neurons recorded in different subjects
    (except in important invertebrate model organisms). However, recent progress
    has shown that we can often extract population-level statistical features
    from large-scale neural recording traces, which replicate (at least qualitatively)
    across subjects (see, e.g.,
    <a href="https://stat.columbia.edu/~cunningham/pdf/ChurchlandNature2012.pdf", target="_blank">Churchland et al., 2012</a>).
    I'm interested in developing statistical methods that formalize these observations
    and using these new tools to perform larger-scale comparative analyses across cohorts
    containing hundreds of experimental subjects.
- items:
  - name: Universality &amp; Individuality in Artificial Networks
    icon: file-pdf-o
    url: http://alexhwilliams.info/pdf/2019_univ_and_indiv.pdf
    description: >
      I worked with <a href="https://niru.dev/", target="_blank">Niru Maheswaranathan</a>
      and <a href="https://scholar.google.com/citations?user=ebBgMSkAAAAJ", target="_blank">David Sussillo</a>
      to study these questions across large populations of trained recurrent neural networks (RNNs).
      On simple tasks that were amenable to low-dimensional visualization and interpretation,
      we found that different network architectures (e.g. vanilla RNNs vs. LSTMs) often showed
      qualitatively similar computational mechanisms (loosely, "universality"). However, we
      also found differences ("individuality") in the geometric structure of different solutions.
      We expect that we will need to wrestle with similar outcomes when these comparative
      analyses are brought to bear on biological data.
  - name: Distance Metrics for Comparing Neural Representations
    icon: youtube-square
    url: https://youtu.be/Lt_Vo-tQcW0
    description: >
      I have recently been interested in revisiting some of the foundational approaches
      to quantifying similarity across neurocomputational systems. I describe some of
      the high-level ideas in the video above, and I have shared a small code package
      <a href="https://github.com/ahwillia/netrep" target="_blank">on github</a>.
      A forthcoming manuscript will provide more details and specifics.
