- title: Unsupervised Learning Techniques for Large-Scale, Multi-Trial Neural Data
  description: >
    An increasingly common paradigm in neuroscience is to simultaneously record the
    activity of many neurons over repeated experimental trials (e.g., multiple
    presentations of a sensory stimulus, or a repeated motor action). The resulting
    datasets can be very large (in some cases, containing thousands of neurons and trials).
    I'm interested in finding general statistical approaches for understanding datasets
    of this form.
  items:
  - name: Tensor Components Analysis (TCA)
    icon: file-pdf-o
    url: http://alexhwilliams.info/pdf/cosyne17.pdf
    description: >
      Commonly used methods for dimensionality reduction (such as <a
      href="http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/" target="_blank">PCA</a>)
      identify low-dimensional features of within-trial neural dynamics, but do not model
      changes in neural activity across trials. To better understand processes like learning
      and trial-to-trial variability, I'm exploring <a
      href="http://www.sandia.gov/~tgkolda/pubs/pubfiles/TensorReview.pdf" target="_blank">
      tensor decomposition methods</a> to find reduced representations of multi-trial datasets.
  - name: TensorTools
    icon: github-square
    url: http://github.com/ahwillia/tensortools
    description: >
      My Python toolbox for fitting tensor decompositions to neural data.
  - name: Time Warped Dimensionality Reduction
    icon: github-square
    url: https://github.com/ganguli-lab/twpca
    description: >
      Analysis of neural data often relies on a strict alignment of neural activity to
      a stimulus or behavioral event on each trial. However, alignment to external events
      is not always possible (e.g., in cases where neural activity is locked to internal
      cognitive states or decisions or other unobserved latent factors). I've developed a
      method to align trials by time warping, while jointly fitting a dimensionality
      reduction model. This was done in close collaboration with
      <a href="cs.stanford.edu/~poole/">Ben Poole</a> and
      <a href="http://niru.org/">Niru Maheswaranathan</a>.

- title: Theoretical Molecular Neurobiology
  description: >
    Biology computes with both electrical and biochemical signals. I'm interested in modeling
    the interface of these two substrates of computation.
  items:
  - name: Homeostatic Plasticity
    icon: file-pdf-o
    url: http://alexhwilliams.info/pdf/OLeary_etal_2014.pdf
    description: >
      Neurons alter ion channel and synaptic receptor expression/activity to maintain activity
      levels in physiologically stable regimes. This can be modeled from a control theoretic
      perspective, which provides perspectives on how noisy molecular processes can nevertheless
      support reliable physiological behaviors.
  - name: Microtubular Transport in Complex Dendritic Trees
    icon: file-pdf-o
    url: http://alexhwilliams.info/pdf/Williams_etal_2016.pdf
    description: >
      Neurons are remarkably complex cells. Given this, it seems an almost insurmountable challenge
      to transport molecular cargo reliably. I've studied a few simple models of how reliable
      transport can be accomplished.
  - name: PyNeuronToolbox
    icon: github-square
    description: >
      A package I wrote to enable better <a href="https://www.neuron.yale.edu/neuron/"
      target="_blank">NEURON</a> simulations in Jupyter notebooks.
    url: https://github.com/ahwillia/PyNeuron-Toolbox

# - title: Variability and Degeneracy in High-Dimensional Systems
#   description: >
#     Sloppy models..
#   items:
#   - name: Homeostatic Plasticity
#     icon: file-pdf-o
#     url: http://alexhwilliams.info/pdf/OLeary_etal_2014.pdf
#     description: >
#       test ets
#   - name: Sloppy Error Landscapes
#     icon: edit
#     url: http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/
#     description: >
#       Sloppy models overview

- title: Miscellaneous Codes
  description: >
    I've written a few packages in Julia for statistics and optimization.
    These are not in active development at the moment, but I'd like to return
    to them someday.

  items:
  - name: Einsum.jl
    icon: github-square
    description: >
      Einstein summation notation for flexible and efficient multi-dimensional arrray
      computations.
    url: https://github.com/ahwillia/Einsum.jl

  - name: NonNegLeastSquares.jl
    icon: github-square
    description: >
      Active-set methods to efficiently solve
      <a href="https://en.wikipedia.org/wiki/Non-negative_least_squares"
      target="_blank">nonnegative least-squares problems</a>.
    url: https://github.com/ahwillia/NonNegLeastSquares.jl

  - name: ToyHMM.jl
    icon: github-square
    description: >
      A simple package for fitting hidden markov models with discrete emissions. Good for teaching!
    url: https://github.com/ahwillia/HiddenMarkovModel.jl

